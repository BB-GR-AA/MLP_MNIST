{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_MNIST_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n56ChGSi7ST",
        "colab_type": "text"
      },
      "source": [
        "# Multilayer Perceptrion (MLP)\n",
        "\n",
        "The objective of the program is to utilize 2 layers, each one a linear transformations of the form \\\\\n",
        "$T: \\mathbf{y} = \\mathbf{X}\\mathbf{w}+\\mathbf{b}$ \\\\\n",
        "to recognize numbers from images.\n",
        "\n",
        "# Google Colab \n",
        "Suggested tool. \\\\\n",
        "Ready to work, no instalation, no extra work, notebook.\n",
        "Do not know how to insert figures yet...\n",
        "\n",
        "# MNIST dataset\n",
        "\n",
        "![texto alternativo](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png) \\\\\n",
        "Source: https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png \\\\\n",
        "\n",
        "Greyscale 28x28 pixels (values from 0 to 255) \\\\\n",
        "60,000 training pairs, labeled from 0 to 9 \\\\\n",
        "10,000 testing pairs \\\\\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQMm7QsEyg3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCKmCn8RyqrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "19434dfa-93dc-4155-9839-ced0adcaf910"
      },
      "source": [
        "# Loading and transforming the data into Pytorch tensors\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
        "# According to M. H. Burle, the mean and standard deviation of the MNIST training data are 0.1307 and 0.3081 respectively, hence these values\n",
        "\n",
        "train_data = datasets.MNIST('',train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('',train=False, download=True, transform=transform)\n",
        "\n",
        "print(f\"train_data elements: {len(train_data)}\")\n",
        "print(f\"test_data elements: {len(test_data)}\")\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=True)\n",
        "dataiter = iter(train_loader)\n",
        "batchimg, batchlabel = dataiter.next()\n",
        "\n",
        "batchplot = plt.figure(figsize=(20, 5))\n",
        "for i in torch.arange(20):\n",
        "    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n",
        "    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n",
        "    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_data elements: 60000\n",
            "test_data elements: 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x360 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l37zj4I8jhuV",
        "colab_type": "text"
      },
      "source": [
        "# NN structure\n",
        "\n",
        "Two hidden layers\n",
        "\n",
        "https://drive.google.com/file/d/1bjzN-nEp0Jcx5vdyRVozLlmDXwptWd4p/view?usp=sharing\n",
        "\n",
        "784 pixels $\\xrightarrow{T_1}$ M neurons $\\xrightarrow{T_1}$ 10 neurons\n",
        "\n",
        "In Gonzalo's example we were using SGD to find the weights in \\\\\n",
        "$T: \\mathbf{y} = \\mathbf{X}\\mathbf{w}+\\mathbf{b}$ \\\\\n",
        "Here, we do that twice\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqSMDodP2b35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class torch.nn.Module\n",
        "# is the \"Base class for all neural network modules\"\n",
        "# This is how we defin the network architecture\n",
        "\n",
        "class Net(nn.Module): \n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(784,128) # fully connected\n",
        "    self.fc2 = nn.Linear(128,10) # 128 is arbitrary\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x,1) #transforms a square, cube, etc into a line\n",
        "    x = self.fc1(x) # fully connected\n",
        "    x = F.relu(x)   # activation function\n",
        "    x = self.fc2(x) # fully connected\n",
        "    output = F.log_softmax(x, dim=1) #threshold ?\n",
        "    return output\n",
        "\n",
        "model = Net()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIJ7anSh5ta9",
        "colab_type": "text"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "We will define the training and testing processes as functions so we can call them iteratively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojoyHjr15s7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer  \n",
        "#optimizer = optim.SGD([w_hat, b_hat], lr=alpha) # Holds current state of parameters and updates based on gradients.\n",
        "optimizer = optim.Adadelta(model.parameters(),lr=1.0)\n",
        "optimizer.zero_grad()                                         # Set the gradients to zero.   \n",
        "optimizer.step() \n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "# Testing function \n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6NgjDPw6HR1",
        "colab_type": "text"
      },
      "source": [
        "# Main program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLT_dUBXiwRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6fcb95f-1e8f-45cb-b75a-ea27344f5bdf"
      },
      "source": [
        "epochs = 3\n",
        "torch.manual_seed(1)\n",
        "device = torch.device('cpu')\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316140\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.872084\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.474214\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.456610\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.334980\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.333797\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.189549\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.301503\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.596658\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.207870\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.240116\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.405099\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.297655\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.148928\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.224455\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.124125\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.222931\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.155021\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.457864\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.249751\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.194445\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.126571\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.213094\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.393816\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.189804\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.440356\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.196590\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.119699\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.123308\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.220268\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.226656\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.182406\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.089379\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.204259\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.027346\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.094041\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.157223\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.282247\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.059659\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.100363\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.108003\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.190146\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.321741\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.172838\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.075830\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.131242\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.147857\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.188525\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.166407\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.236338\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.284615\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.281805\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.249716\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.040865\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.156852\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.204351\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.087590\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.098337\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.185333\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.155979\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.096462\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.020350\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.094090\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.110279\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.228323\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.119313\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.039586\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.234798\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.182875\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.062392\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.190290\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.196342\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.355883\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.209821\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.115525\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.064199\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.085827\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.081258\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.059085\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.192532\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.141079\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.098474\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.019258\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.139364\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.092373\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.087022\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.062363\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.151085\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.075473\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.120747\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.187682\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.029798\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.009983\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.012790\n",
            "\n",
            "Test set: Average loss: 0.1250, Accuracy: 9617/10000 (96%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.064883\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.100139\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.089166\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.119994\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.038903\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.047069\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.026627\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.085245\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.135393\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.094605\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.101919\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.118831\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.124415\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.069948\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.085034\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.046217\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.069526\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.035520\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.109856\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.050548\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.070346\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.027788\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.055707\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.160278\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.064176\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.127566\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.097093\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.022329\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.031281\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.115618\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.122680\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.083454\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.054125\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.101676\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.012768\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.038196\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.099831\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.128102\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.018204\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.021781\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.052947\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.057120\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.297777\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.122312\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.026768\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.046243\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.097801\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.068948\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.051355\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.066376\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.152433\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.129361\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.184214\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.017576\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.073520\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.089082\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.062423\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.043977\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.059767\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.125014\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.030463\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.006702\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.025587\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.082695\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.174414\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.033383\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.012984\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.122434\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.080310\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.019535\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.122487\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.072624\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.253738\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.125606\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.058540\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.033095\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.036152\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.030304\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.016037\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.059670\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.082422\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.026855\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.006316\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.055210\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.040534\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.035554\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.032606\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.089308\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.057115\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.034910\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.149583\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.011055\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.005035\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.001459\n",
            "\n",
            "Test set: Average loss: 0.0915, Accuracy: 9730/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.041971\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.077813\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.024869\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.078368\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.045025\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.011156\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.004576\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.033947\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.084332\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.098616\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.091979\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.098686\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.082615\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.039449\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.063647\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.042305\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.052023\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.013684\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.062404\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.030520\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.076477\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.008257\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.021941\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.096933\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.037759\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.072297\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.065215\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.005106\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.024681\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.064189\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.083764\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.062134\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.026763\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.051400\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.004298\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.024556\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.027332\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.088345\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.013036\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.008961\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.038938\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.030885\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.268589\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.061986\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.018882\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.026432\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.069195\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.032363\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.032933\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.020032\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.084787\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.070483\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.120719\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.011392\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.048736\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.067282\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.034350\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.025314\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.028884\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.124658\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.013852\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.011048\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.032087\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.048085\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.133797\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.019047\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.009986\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.077579\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.047520\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.009485\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.075190\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.053939\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.144591\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.112138\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.043926\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.014670\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.023454\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.014174\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.016697\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.013263\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.080611\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.013000\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.004853\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.023381\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.017968\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.020362\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.025467\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.066130\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.036490\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.009937\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.091158\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.003640\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.003695\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.000667\n",
            "\n",
            "Test set: Average loss: 0.0861, Accuracy: 9753/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}